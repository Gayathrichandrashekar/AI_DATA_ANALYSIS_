{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Handling Schema Mismatches using Spark\n",
    "**Description**: Use Apache Spark to address schema mismatches by transforming data to match\n",
    "the expected schema.\n",
    "\n",
    "**Steps**:\n",
    "1. Create Spark session\n",
    "2. Load dataframe\n",
    "3. Define the expected schema\n",
    "4. Handle schema mismatches\n",
    "5. Show corrected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StructType, StructField, IntegerType, DoubleType, StringType\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Step 1: Create Spark session\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, StringType\n",
    "\n",
    "# Step 1: Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Schema Mismatches Handling\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 2: Load DataFrame (Assuming CSV input for this example)\n",
    "data = [\n",
    "    (25, 50000, 'HR'),\n",
    "    (27, 54000, 'Finance'),\n",
    "    (None, 58000, 'HR'),\n",
    "    (29, None, 'Finance'),\n",
    "    (30, 62000, 'IT'),\n",
    "    (None, None, None)\n",
    "]\n",
    "\n",
    "# Define the schema explicitly\n",
    "schema = StructType([\n",
    "    StructField(\"Age\", IntegerType(), True),\n",
    "    StructField(\"Salary\", DoubleType(), True),\n",
    "    StructField(\"Department\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame using the schema\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Step 3: Define the expected schema (no mismatches in this case)\n",
    "expected_schema = StructType([\n",
    "    StructField(\"Age\", IntegerType(), True),\n",
    "    StructField(\"Salary\", DoubleType(), True),\n",
    "    StructField(\"Department\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Step 4: Handle schema mismatches by casting columns to the expected data types\n",
    "df_corrected = df.select(\n",
    "    col(\"Age\").cast(IntegerType()).alias(\"Age\"),\n",
    "    col(\"Salary\").cast(DoubleType()).alias(\"Salary\"),\n",
    "    col(\"Department\").cast(StringType()).alias(\"Department\")\n",
    ")\n",
    "\n",
    "# Show the schema after casting and the corrected data\n",
    "df_corrected.printSchema()\n",
    "df_corrected.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Detect and Correct Incomplete Data in ETL\n",
    "**Description**: Use Python and Pandas to detect incomplete data in an ETL process and fill\n",
    "missing values with estimates.\n",
    "\n",
    "**Steps**:\n",
    "1. Detect incomplete data\n",
    "2. Fill missing values\n",
    "3. Report changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code from here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample data with missing values\n",
    "data = {\n",
    "    'Age': [25, 27, np.nan, 29, 30],\n",
    "    'Salary': [50000, 54000, 58000, np.nan, 62000],\n",
    "    'Department': ['HR', 'Finance', 'HR', np.nan, 'IT']\n",
    "}\n",
    "\n",
    "# Step 1: Load the data into a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Show original data\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Step 2: Detect and fill missing values\n",
    "# Impute numerical columns with mean for 'Age' and 'Salary'\n",
    "df['Age'] = df['Age'].fillna(df['Age'].mean())\n",
    "df['Salary'] = df['Salary'].fillna(df['Salary'].mean())\n",
    "\n",
    "# Impute categorical column 'Department' with the mode (most frequent value)\n",
    "df['Department'] = df['Department'].fillna(df['Department'].mode()[0])\n",
    "\n",
    "# Step 3: Report the changes\n",
    "print(\"\\nDataFrame after Imputation:\")\n",
    "print(df)\n",
    "\n",
    "# Report changes: Display which columns had missing values and how many were filled\n",
    "missing_report = {\n",
    "    'Age': df['Age'].isnull().sum(),\n",
    "    'Salary': df['Salary'].isnull().sum(),\n",
    "    'Department': df['Department'].isnull().sum()\n",
    "}\n",
    "\n",
    "print(\"\\nMissing Values Report:\")\n",
    "print(missing_report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Original Dataset:\n",
      "      Name    Age         City   Income  Gender\n",
      "0    Alice   25.0     New York    50000       F\n",
      "1      Bob    NaN  Los Angeles    60000       M\n",
      "2  Charlie   30.0     New York    52000       M\n",
      "3     None   22.0      Chicago    48000       F\n",
      "4      Eve   28.0         None      nan       F\n",
      "5    Frank  120.0        Miami  1000000    Male\n",
      "6    Grace   27.0       Boston    49000  Female\n",
      "\n",
      "âœ… Preprocessed Dataset:\n",
      "      Name   Age    Income  City_Boston  City_Chicago  City_Los Angeles  \\\n",
      "0    Alice  0.15  0.012358        False         False             False   \n",
      "1      Bob  1.00  0.074150        False         False              True   \n",
      "2  Charlie  0.40  0.024717        False         False             False   \n",
      "3  Unknown  0.00  0.000000        False          True             False   \n",
      "4      Eve  0.30  1.000000        False         False             False   \n",
      "5    Frank  0.30  0.024717        False         False             False   \n",
      "6    Grace  0.25  0.006179         True         False             False   \n",
      "\n",
      "   City_Miami  City_New York  City_Unknown  Gender_F  Gender_M  \n",
      "0       False           True         False      True     False  \n",
      "1       False          False         False     False      True  \n",
      "2       False           True         False     False      True  \n",
      "3       False          False         False      True     False  \n",
      "4       False          False          True      True     False  \n",
      "5        True          False         False     False      True  \n",
      "6       False          False         False      True     False  \n"
     ]
    }
   ],
   "source": [
    "# Steps in Data Preprocessing\n",
    "\n",
    "# 1. Data Collection: Gathering raw data from various sources.\n",
    "# Task 1: Collect data from two different sources and merge them.\n",
    "# Task 2: Validate the integrity of the collected datasets.\n",
    "# Task 3: Reflect on challenges faced during data collection and how they were addressed.\n",
    "\n",
    "\n",
    "# Ques_3.ipynb\n",
    "# Module 5: Introduction to Data Preprocessing & Cleaning\n",
    "# Task: Steps in Data Preprocessing\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load Data\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', None, 'Eve', 'Frank', 'Grace'],\n",
    "    'Age': [25, np.nan, 30, 22, 28, 120, 27],\n",
    "    'City': ['New York', 'Los Angeles', 'New York', 'Chicago', None, 'Miami', 'Boston'],\n",
    "    'Income': ['50000', '60000', '52000', '48000', 'nan', '1000000', '49000'],\n",
    "    'Gender': ['F', 'M', 'M', 'F', 'F', 'Male', 'Female']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"ðŸ”¹ Original Dataset:\")\n",
    "print(df)\n",
    "\n",
    "# Step 2: Handle Missing Values\n",
    "df['Name'].fillna('Unknown', inplace=True)\n",
    "df['Age'].fillna(df['Age'].mean(), inplace=True)\n",
    "df['City'].fillna('Unknown', inplace=True)\n",
    "df['Income'].replace('nan', np.nan, inplace=True)\n",
    "df['Income'] = df['Income'].astype(float)\n",
    "df['Income'].fillna(df['Income'].mean(), inplace=True)\n",
    "\n",
    "# Step 3: Remove Duplicates (if any)\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Step 4: Fix Incorrect Data (e.g., outlier in Age and Income)\n",
    "# Using IQR for Age\n",
    "Q1_age = df['Age'].quantile(0.25)\n",
    "Q3_age = df['Age'].quantile(0.75)\n",
    "IQR_age = Q3_age - Q1_age\n",
    "upper_age = Q3_age + 1.5 * IQR_age\n",
    "df['Age'] = df['Age'].apply(lambda x: df['Age'].median() if x > upper_age else x)\n",
    "\n",
    "# Using IQR for Income\n",
    "Q1_inc = df['Income'].quantile(0.25)\n",
    "Q3_inc = df['Income'].quantile(0.75)\n",
    "IQR_inc = Q3_inc - Q1_inc\n",
    "upper_inc = Q3_inc + 1.5 * IQR_inc\n",
    "df['Income'] = df['Income'].apply(lambda x: df['Income'].median() if x > upper_inc else x)\n",
    "\n",
    "# Step 5: Standardize Categorical Values\n",
    "df['Gender'] = df['Gender'].str.upper().replace({'MALE': 'M', 'FEMALE': 'F'})\n",
    "\n",
    "# Step 6: Encode Categorical Variables (if required)\n",
    "df_encoded = pd.get_dummies(df, columns=['City', 'Gender'])\n",
    "\n",
    "# Step 7: Normalize/Scale Numerical Features (optional)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df_encoded[['Age', 'Income']] = scaler.fit_transform(df_encoded[['Age', 'Income']])\n",
    "\n",
    "# Final Preprocessed Data\n",
    "print(\"\\nâœ… Preprocessed Dataset:\")\n",
    "print(df_encoded)\n",
    "\n",
    "# Optional: Save preprocessed data\n",
    "# df_encoded.to_csv(\"preprocessed_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Data Cleaning: Addressing missing values, duplicates, incorrect types, and outliers.\n",
    "# Task 1: Clean a given dataset and document the changes made.\n",
    "# Task 2: Create a checklist to ensure comprehensive data cleaning in future projects.\n",
    "# Task 3: Collaborate with a peer to clean a new dataset and present your solutions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Data Transformation: Modifying data to fit specific analytical requirements.\n",
    "# Task 1: Transform a date column into separate 'day', 'month', and 'year' columns.\n",
    "# Task 2: Apply normalization to a dataset feature and confirm the changes.\n",
    "# Task 3: Discuss the importance of data transformation in model interpretability.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Feature Scaling: Adjusting data features to a common scale.\n",
    "# Task 1: Apply Min-Max scaling to a dataset.\n",
    "# Task 2: Standardize a dataset and visualize the changes with a histogram.\n",
    "# Task 3: Analyze how feature scaling impacts the performance of different machine learning algorithms.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Feature Engineering: Creating new features from existing ones to improve model accuracy.\n",
    "# Task 1: Create a new synthetic feature from existing dataset features.\n",
    "# Task 2: Evaluate the impact of new features on model accuracy.\n",
    "# Task 3: Read an academic paper on feature engineering techniques and present the findings.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Task: Complete Pipeline for a Dataset\n",
    "1. Objective: Build a complex pipeline with multiple transformations.\n",
    "2. Steps:\n",
    "    - Load a sample dataset.\n",
    "    - Define a transformation pipeline with both imputation and scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Test for Imputation\n",
    "def test_imputation():\n",
    "    # Sample data with missing values\n",
    "    data = {'A': [1, 2, np.nan, 4], 'B': [5, np.nan, 7, 8]}\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    imputed_data = imputer.fit_transform(df)\n",
    "    \n",
    "    # Assert that missing values are replaced\n",
    "    assert not np.any(np.isnan(imputed_data)), \"Missing values exist after imputation!\"\n",
    "\n",
    "# Test for Scaling\n",
    "def test_scaling():\n",
    "    # Sample data\n",
    "    data = {'A': [1, 2, 3, 4], 'B': [5, 6, 7, 8]}\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(df)\n",
    "    \n",
    "    # Assert that the scaled data has mean=0 and std=1 for each column\n",
    "    assert np.abs(np.mean(scaled_data[:, 0])) < 0.1, \"Column A mean is not close to 0\"  # Column A\n",
    "    assert np.abs(np.mean(scaled_data[:, 1])) < 0.1, \"Column B mean is not close to 0\"  # Column B\n",
    "    assert np.abs(np.std(scaled_data[:, 0]) - 1) < 0.1, \"Column A std is not close to 1\"\n",
    "    assert np.abs(np.std(scaled_data[:, 1]) - 1) < 0.1, \"Column B std is not close to 1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data with Missing Values:\n",
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0                NaN               3.5                1.4               0.2\n",
      "1                4.9               3.0                1.4               0.2\n",
      "2                4.7               3.2                1.3               0.2\n",
      "3                4.6               3.1                1.5               0.2\n",
      "4                5.0               3.6                1.4               0.2\n",
      "\n",
      "Processed Data (Imputed and Scaled):\n",
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0           0.000000          1.019004          -1.354309         -1.315444\n",
      "1          -1.152203         -0.131979          -1.354309         -1.315444\n",
      "2          -1.395201          0.328414          -1.411410         -1.315444\n",
      "3          -1.516700          0.098217          -1.297209         -1.315444\n",
      "4          -1.030704          1.249201          -1.354309         -1.315444\n"
     ]
    }
   ],
   "source": [
    "# Task: Imputation Function\n",
    "\n",
    "# Task: Imputation Function\n",
    "\n",
    "\n",
    "\n",
    "def impute_data(df):\n",
    "    \"\"\"Impute missing values in the dataset using SimpleImputer with mean strategy.\"\"\"\n",
    "    imputer = SimpleImputer(strategy='mean')  # Impute missing values with the column mean\n",
    "    imputed_data = imputer.fit_transform(df)\n",
    "    return pd.DataFrame(imputed_data, columns=df.columns)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Scaling Function\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Combined Transformation Function\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# --- Imputation Function ---\n",
    "def impute_data(df):\n",
    "    \"\"\"Impute missing values in the dataset using SimpleImputer with mean strategy.\"\"\"\n",
    "    imputer = SimpleImputer(strategy='mean')  # Impute missing values with the column mean\n",
    "    imputed_data = imputer.fit_transform(df)\n",
    "    return pd.DataFrame(imputed_data, columns=df.columns)\n",
    "\n",
    "# --- Scaling Function ---\n",
    "def scale_data(df):\n",
    "    \"\"\"Scale the numerical data using StandardScaler.\"\"\"\n",
    "    scaler = StandardScaler()  # Standardize features by removing the mean and scaling to unit variance\n",
    "    scaled_data = scaler.fit_transform(df)\n",
    "    return pd.DataFrame(scaled_data, columns=df.columns)\n",
    "\n",
    "# --- Combined Transformation Function ---\n",
    "def combined_transformation(df):\n",
    "    \"\"\"Apply both imputation and scaling to the dataset.\"\"\"\n",
    "    # Step 1: Impute missing values\n",
    "    df_imputed = impute_data(df)\n",
    "    \n",
    "    # Step 2: Scale the features\n",
    "    df_scaled = scale_data(df_imputed)\n",
    "    \n",
    "    return df_scaled\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "\n",
    "# Introduce missing values for demonstration\n",
    "df.iloc[0, 0] = np.nan  # Missing value in the first feature (Feature1)\n",
    "df.iloc[5, 2] = np.nan  # Missing value in the third feature (Feature3)\n",
    "\n",
    "# Display original data with missing values\n",
    "print(\"Original Data with Missing Values:\")\n",
    "print(df.head())\n",
    "\n",
    "# Apply the combined transformation\n",
    "processed_df = combined_transformation(df)\n",
    "\n",
    "# Show the processed dataset after imputation and scaling\n",
    "print(\"\\nProcessed Data (Imputed and Scaled):\")\n",
    "print(processed_df.head())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Scaling Function\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Combined Transformation Function\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

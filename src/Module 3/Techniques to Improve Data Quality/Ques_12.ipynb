{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gfhfh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Original Data:\n",
      "     ID     Name  Age     City\n",
      "0  101    Alice   25       NY\n",
      "1  102      Bob   30       LA\n",
      "2  103  Charlie   35  Chicago\n",
      "3  104    David   40  Houston\n",
      "4  105      Eva   28       LA\n",
      "5  101    Alice   25       NY\n",
      "6  103  Charlie   35  Chicago\n",
      "7  106    Frank   33  Seattle\n",
      "8  107    Grace   45   Boston\n",
      "9  107    Grace   45   Boston\n",
      "\n",
      "üî∏ Full Row Duplicates:\n",
      "     ID     Name  Age     City\n",
      "0  101    Alice   25       NY\n",
      "2  103  Charlie   35  Chicago\n",
      "5  101    Alice   25       NY\n",
      "6  103  Charlie   35  Chicago\n",
      "8  107    Grace   45   Boston\n",
      "9  107    Grace   45   Boston\n",
      "\n",
      "‚úÖ After Removing Full Row Duplicates:\n",
      "     ID     Name  Age     City\n",
      "0  101    Alice   25       NY\n",
      "1  102      Bob   30       LA\n",
      "2  103  Charlie   35  Chicago\n",
      "3  104    David   40  Houston\n",
      "4  105      Eva   28       LA\n",
      "7  106    Frank   33  Seattle\n",
      "8  107    Grace   45   Boston\n",
      "\n",
      "üî∏ Partial Duplicates Based on 'ID':\n",
      " Empty DataFrame\n",
      "Columns: [ID, Name, Age, City]\n",
      "Index: []\n",
      "\n",
      "‚úÖ After Removing Duplicate 'ID's (Keep First):\n",
      "     ID     Name  Age     City\n",
      "0  101    Alice   25       NY\n",
      "1  102      Bob   30       LA\n",
      "2  103  Charlie   35  Chicago\n",
      "3  104    David   40  Houston\n",
      "4  105      Eva   28       LA\n",
      "7  106    Frank   33  Seattle\n",
      "8  107    Grace   45   Boston\n",
      "\n",
      "üî∏ Data with 'is_duplicate' flag:\n",
      "     ID     Name  Age     City  is_duplicate\n",
      "0  101    Alice   25       NY         False\n",
      "1  102      Bob   30       LA         False\n",
      "2  103  Charlie   35  Chicago         False\n",
      "3  104    David   40  Houston         False\n",
      "4  105      Eva   28       LA         False\n",
      "7  106    Frank   33  Seattle         False\n",
      "8  107    Grace   45   Boston         False\n",
      "‚úÖ Cleaned data saved to: Q12_cleaned_deduplicated.csv\n"
     ]
    }
   ],
   "source": [
    "# Ques_12.ipynb - Dealing with Duplicates & Redundancy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# === Step 1: Generate or Load Data ===\n",
    "def generate_sample_data():\n",
    "    \"\"\"Generates sample data with intentional duplicates.\"\"\"\n",
    "    data = {\n",
    "        'ID': [101, 102, 103, 104, 105, 101, 103, 106, 107, 107],\n",
    "        'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva', 'Alice', 'Charlie', 'Frank', 'Grace', 'Grace'],\n",
    "        'Age': [25, 30, 35, 40, 28, 25, 35, 33, 45, 45],\n",
    "        'City': ['NY', 'LA', 'Chicago', 'Houston', 'LA', 'NY', 'Chicago', 'Seattle', 'Boston', 'Boston']\n",
    "    }\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# === Step 2: Validate Data ===\n",
    "def validate_data(df):\n",
    "    \"\"\"Basic sanity checks for input DataFrame.\"\"\"\n",
    "    if df.empty:\n",
    "        raise ValueError(\"DataFrame is empty. Please check the input source.\")\n",
    "    if df.isnull().all().any():\n",
    "        print(\"‚ö†Ô∏è Warning: Some columns have only missing values.\")\n",
    "    if 'ID' not in df.columns:\n",
    "        raise KeyError(\"Missing required column: 'ID'\")\n",
    "    return True\n",
    "\n",
    "\n",
    "# === Step 3: Identify Full Duplicates ===\n",
    "def get_full_duplicates(df):\n",
    "    return df[df.duplicated(keep=False)]\n",
    "\n",
    "\n",
    "# === Step 4: Remove Full Duplicates ===\n",
    "def remove_full_duplicates(df):\n",
    "    return df.drop_duplicates()\n",
    "\n",
    "\n",
    "# === Step 5: Identify Partial Duplicates by Column (e.g., ID) ===\n",
    "def get_partial_duplicates(df, subset_col):\n",
    "    return df[df.duplicated(subset=[subset_col], keep=False)]\n",
    "\n",
    "\n",
    "# === Step 6: Remove Partial Duplicates by Column (keeping first) ===\n",
    "def drop_partial_duplicates(df, subset_col):\n",
    "    return df.drop_duplicates(subset=[subset_col], keep='first')\n",
    "\n",
    "\n",
    "# === Step 7: Flag Duplicates in a Column ===\n",
    "def flag_duplicates(df, subset_col):\n",
    "    df = df.copy()\n",
    "    df['is_duplicate'] = df.duplicated(subset=[subset_col], keep=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "# === Step 8: Save to CSV ===\n",
    "def save_cleaned_data(df, path=\"Q12_cleaned_deduplicated.csv\"):\n",
    "    df.to_csv(path, index=False)\n",
    "    print(f\"‚úÖ Cleaned data saved to: {path}\")\n",
    "\n",
    "\n",
    "# === MAIN EXECUTION ===\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Load Data\n",
    "    df = generate_sample_data()\n",
    "    print(\"üîπ Original Data:\\n\", df)\n",
    "\n",
    "    # Step 2: Validate\n",
    "    try:\n",
    "        validate_data(df)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Validation Error: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    # Step 3: Display full duplicates\n",
    "    full_dupes = get_full_duplicates(df)\n",
    "    print(\"\\nüî∏ Full Row Duplicates:\\n\", full_dupes)\n",
    "\n",
    "    # Step 4: Remove full duplicates\n",
    "    df_no_full_dupes = remove_full_duplicates(df)\n",
    "    print(\"\\n‚úÖ After Removing Full Row Duplicates:\\n\", df_no_full_dupes)\n",
    "\n",
    "    # Step 5: Show partial duplicates based on 'ID'\n",
    "    partial_dupes = get_partial_duplicates(df_no_full_dupes, 'ID')\n",
    "    print(\"\\nüî∏ Partial Duplicates Based on 'ID':\\n\", partial_dupes)\n",
    "\n",
    "    # Step 6: Drop partial duplicates (keep first ID)\n",
    "    df_unique_ids = drop_partial_duplicates(df_no_full_dupes, 'ID')\n",
    "    print(\"\\n‚úÖ After Removing Duplicate 'ID's (Keep First):\\n\", df_unique_ids)\n",
    "\n",
    "    # Step 7: Add a duplicate flag for transparency\n",
    "    df_flagged = flag_duplicates(df_no_full_dupes, 'ID')\n",
    "    print(\"\\nüî∏ Data with 'is_duplicate' flag:\\n\", df_flagged)\n",
    "\n",
    "    # Step 8: Save cleaned output\n",
    "    save_cleaned_data(df_unique_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
